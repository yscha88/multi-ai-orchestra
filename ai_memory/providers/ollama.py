"""
Ollama Local LLM Provider êµ¬í˜„ - ì„œë²„ ìë™ ê´€ë¦¬ ë° CLI í†µí•©
"""

import requests
import json
import time
import subprocess
import os
import signal
import threading
from typing import List, Dict, Any, Optional
from pathlib import Path

from .base import BaseProvider, ChatMessage, ChatResponse, ModelInfo, ProviderType
from .base import ProviderError, ProviderUnavailableError, ProviderConfigError

class OllamaProvider(BaseProvider):
    """Ollama Local LLM Provider - ì„œë²„ ìë™ ê´€ë¦¬"""

    def __init__(self, url: str = "http://localhost:11434",
                 model_name: str = "llama3.1:8b",
                 auto_start_server: bool = True, **kwargs):
        super().__init__(model_name, **kwargs)

        self.url = url.rstrip('/')
        self.api_url = f"{self.url}/api"
        self.auto_start_server = auto_start_server
        self.server_process = None

        # ëª¨ë¸ ì •ë³´ ì„¤ì •
        self._model_info = ModelInfo(
            name=model_name,
            provider="ollama",
            type=ProviderType.LOCAL_LLM,
            max_tokens=self._get_model_context_length(model_name),
            cost_per_1k_tokens=0.0,
            supports_streaming=True,
            supports_function_calling=False,
            metadata={
                "url": self.url,
                "local": True,
                "model_family": self._get_model_family(model_name),
                "auto_start_server": auto_start_server
            }
        )

    def _get_model_family(self, model_name: str) -> str:
        """ëª¨ë¸ íŒ¨ë°€ë¦¬ ì¶”ì¶œ"""
        if "llama" in model_name.lower():
            return "llama"
        elif "codellama" in model_name.lower():
            return "codellama"
        elif "phi" in model_name.lower():
            return "phi"
        elif "tinyllama" in model_name.lower():
            return "tinyllama"
        elif "mistral" in model_name.lower():
            return "mistral"
        elif "gemma" in model_name.lower():
            return "gemma"
        else:
            return "unknown"

    def _get_model_context_length(self, model_name: str) -> int:
        """ëª¨ë¸ë³„ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶”ì •"""
        model_contexts = {
            "llama": 4096,
            "codellama": 4096,
            "phi": 2048,
            "tinyllama": 2048,
            "mistral": 8192,
            "gemma": 8192
        }

        family = self._get_model_family(model_name)
        return model_contexts.get(family, 2048)

    def _check_ollama_installed(self) -> bool:
        """Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            result = subprocess.run(['ollama', '--version'],
                                  capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False

    def _get_models_via_cli(self) -> List[Dict[str, Any]]:
        """CLIë¥¼ í†µí•´ ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            result = subprocess.run(['ollama', 'list'],
                                  capture_output=True, text=True, timeout=10)

            if result.returncode != 0:
                return []

            models = []
            lines = result.stdout.strip().split('\n')

            # í—¤ë” ë¼ì¸ ê±´ë„ˆë›°ê¸°
            if len(lines) > 1:
                for line in lines[1:]:
                    if line.strip():
                        parts = line.split()
                        if len(parts) >= 3:
                            name = parts[0]
                            model_id = parts[1] if len(parts) > 1 else ""
                            size = parts[2] if len(parts) > 2 else "0"
                            modified = " ".join(parts[3:]) if len(parts) > 3 else ""

                            # í¬ê¸°ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
                            size_bytes = self._parse_size(size)

                            models.append({
                                "name": name,
                                "id": model_id,
                                "size": size_bytes,
                                "modified": modified,
                                "family": self._get_model_family(name),
                                "context_length": self._get_model_context_length(name)
                            })

            return models

        except Exception as e:
            print(f"âš ï¸ CLIë¥¼ í†µí•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def _parse_size(self, size_str: str) -> int:
        """í¬ê¸° ë¬¸ìì—´ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜"""
        if not size_str or size_str == "0":
            return 0

        try:
            size_str = size_str.upper()
            if 'GB' in size_str:
                return int(float(size_str.replace('GB', '')) * 1024**3)
            elif 'MB' in size_str:
                return int(float(size_str.replace('MB', '')) * 1024**2)
            elif 'KB' in size_str:
                return int(float(size_str.replace('KB', '')) * 1024)
            else:
                return int(float(size_str))
        except:
            return 0

    def _start_ollama_server(self) -> bool:
        """Ollama ì„œë²„ ì‹œì‘"""
        if not self._check_ollama_installed():
            print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜ ë°©ë²•: https://ollama.ai/download")
            return False

        try:
            print("ğŸš€ Ollama ì„œë²„ ì‹œì‘ ì¤‘...")

            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„œë²„ ì‹œì‘
            self.server_process = subprocess.Popen(
                ['ollama', 'serve'],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                preexec_fn=os.setsid if os.name != 'nt' else None
            )

            # ì„œë²„ê°€ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
            for i in range(30):
                if self._is_server_running():
                    print("âœ… Ollama ì„œë²„ ì‹œì‘ ì™„ë£Œ")
                    return True
                time.sleep(1)
                if i % 5 == 0:
                    print(f"â³ ì„œë²„ ì‹œì‘ ëŒ€ê¸° ì¤‘... ({i+1}/30)")

            print("âŒ Ollama ì„œë²„ ì‹œì‘ ì‹œê°„ ì´ˆê³¼")
            return False

        except Exception as e:
            print(f"âŒ Ollama ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False

    def _is_server_running(self) -> bool:
        """ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
        try:
            response = requests.get(f"{self.url}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False

    def _stop_ollama_server(self):
        """Ollama ì„œë²„ ì¤‘ì§€"""
        if self.server_process:
            try:
                if os.name == 'nt':  # Windows
                    self.server_process.terminate()
                else:  # Unix-like
                    os.killpg(os.getpgid(self.server_process.pid), signal.SIGTERM)

                self.server_process.wait(timeout=5)
                print("ğŸ›‘ Ollama ì„œë²„ ì¤‘ì§€ë¨")
            except:
                pass
            finally:
                self.server_process = None

    def is_available(self) -> bool:
        """Ollama ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        # 1. ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        if self._is_server_running():
            return True

        # 2. ìë™ ì‹œì‘ì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì‹œì‘ ì‹œë„
        if self.auto_start_server:
            return self._start_ollama_server()

        # 3. ìë™ ì‹œì‘ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ì„¤ì¹˜ ì—¬ë¶€ë§Œ í™•ì¸
        return self._check_ollama_installed()

    def get_available_models(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        # 1. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ API ì‚¬ìš©
        if self._is_server_running():
            try:
                response = requests.get(f"{self.url}/api/tags", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    for model in data.get("models", []):
                        models.append({
                            "name": model["name"],
                            "size": model.get("size", 0),
                            "modified": model.get("modified", ""),
                            "family": self._get_model_family(model["name"]),
                            "context_length": self._get_model_context_length(model["name"])
                        })
                    return models
            except:
                pass

        # 2. API ì‹¤íŒ¨ ì‹œ CLI ì‚¬ìš©
        print("ğŸ”„ CLIë¥¼ í†µí•´ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        return self._get_models_via_cli()

    def get_model_names(self) -> List[str]:
        """ëª¨ë¸ ì´ë¦„ë§Œ ë°˜í™˜"""
        models = self.get_available_models()
        return [model["name"] for model in models]

    def _ensure_server_running(self) -> bool:
        """ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ì‹œì‘"""
        if self._is_server_running():
            return True

        if self.auto_start_server:
            print("ğŸ”„ Ollama ì„œë²„ê°€ ì¤‘ì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¬ì‹œì‘ ì¤‘...")
            return self._start_ollama_server()
        else:
            print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”: ollama serve")
            return False

    def chat(self, messages: List[ChatMessage], **kwargs) -> ChatResponse:
        """Ollamaì™€ ì±„íŒ…"""

        # ì„œë²„ ì‹¤í–‰ í™•ì¸
        if not self._ensure_server_running():
            raise ProviderUnavailableError("Ollama ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ëª¨ë¸ ì¡´ì¬ í™•ì¸
        if not self._model_exists():
            available_models = self.get_model_names()
            if available_models:
                print(f"âŒ ëª¨ë¸ '{self.model_name}'ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models[:3])}")
                print(f"   ì„¤ì¹˜ ëª…ë ¹: ollama pull {self.model_name}")
            else:
                print("âŒ ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("   ëª¨ë¸ ì„¤ì¹˜ ì˜ˆì‹œ: ollama pull llama3.1:8b")
            raise ProviderError(f"ëª¨ë¸ '{self.model_name}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ëª¨ë¸ íŒ¨ë°€ë¦¬ë³„ í”„ë¡¬í”„íŠ¸ ìµœì í™”
        prompt = self._messages_to_prompt(messages)

        start_time = time.time()

        try:
            response = requests.post(
                f"{self.api_url}/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": kwargs.get("temperature", 0.7),
                        "num_predict": kwargs.get("max_tokens", 2000),
                        "top_p": kwargs.get("top_p", 0.9),
                        "stop": kwargs.get("stop", [])
                    }
                },
                timeout=120
            )

            response.raise_for_status()
            result = response.json()

            response_time = time.time() - start_time

            # ì‘ë‹µ ì²˜ë¦¬
            content = result.get("response", "")

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
            input_tokens = len(prompt.split()) * 1.3
            output_tokens = len(content.split()) * 1.3

            token_usage = {
                "input_tokens": int(input_tokens),
                "output_tokens": int(output_tokens)
            }

            return ChatResponse(
                content=content,
                model_info=self._model_info,
                token_usage=token_usage,
                cost_estimate=0.0,
                response_time=response_time,
                metadata={
                    "raw_response": result,
                    "model_family": self._get_model_family(self.model_name),
                    "server_auto_started": self.server_process is not None
                }
            )

        except requests.RequestException as e:
            raise ProviderError(f"Ollama API ì˜¤ë¥˜: {e}")
        except Exception as e:
            raise ProviderError(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    def _model_exists(self) -> bool:
        """ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            available_models = self.get_model_names()
            return self.model_name in available_models
        except:
            return False

    def switch_model(self, new_model_name: str) -> bool:
        """ëª¨ë¸ ë³€ê²½"""
        try:
            available_models = self.get_model_names()
            if new_model_name not in available_models:
                print(f"âŒ ëª¨ë¸ '{new_model_name}'ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print(f"   ì„¤ì¹˜ ëª…ë ¹: ollama pull {new_model_name}")
                return False

            # ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸
            self.model_name = new_model_name
            self._model_info = ModelInfo(
                name=new_model_name,
                provider="ollama",
                type=ProviderType.LOCAL_LLM,
                max_tokens=self._get_model_context_length(new_model_name),
                cost_per_1k_tokens=0.0,
                supports_streaming=True,
                supports_function_calling=False,
                metadata={
                    "url": self.url,
                    "local": True,
                    "model_family": self._get_model_family(new_model_name),
                    "auto_start_server": self.auto_start_server
                }
            )

            return True

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë³€ê²½ ì‹¤íŒ¨: {e}")
            return False

    def get_model_recommendations(self) -> Dict[str, str]:
        """ëª¨ë¸ë³„ ì¶”ì²œ ìš©ë„"""
        return {
            "llama3.1:8b": "ì¼ë°˜ì ì¸ ëŒ€í™” ë° í…ìŠ¤íŠ¸ ìƒì„±",
            "codellama:latest": "ì½”ë”©, í”„ë¡œê·¸ë˜ë° ë„ì›€ (ğŸ”¥ ì½”ë”© ì¶”ì²œ)",
            "phi4:latest": "ë¹ ë¥¸ ì¶”ë¡ , ê°„ë‹¨í•œ ì§ˆë¬¸",
            "tinyllama:latest": "ë§¤ìš° ë¹ ë¥¸ ì‘ë‹µ, ë¦¬ì†ŒìŠ¤ ì ˆì•½",
            "mistral": "ë†’ì€ í’ˆì§ˆì˜ í…ìŠ¤íŠ¸ ìƒì„±",
            "gemma": "êµ¬ê¸€ì˜ ê³ ì„±ëŠ¥ ëª¨ë¸"
        }

    def _messages_to_prompt(self, messages: List[ChatMessage]) -> str:
        """ë©”ì‹œì§€ë“¤ì„ ëª¨ë¸ë³„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
        family = self._get_model_family(self.model_name)

        if family == "codellama":
            return self._codellama_prompt(messages)
        elif family == "phi":
            return self._phi_prompt(messages)
        elif family == "tinyllama":
            return self._tinyllama_prompt(messages)
        else:
            return self._llama_prompt(messages)

    def _codellama_prompt(self, messages: List[ChatMessage]) -> str:
        """CodeLlama ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        prompt_parts = []

        for msg in messages:
            if msg.role == "system":
                prompt_parts.append(f"<s>\n{msg.content}\n</s>")
            elif msg.role == "user":
                prompt_parts.append(f"<user>\n{msg.content}\n</user>")
            elif msg.role == "assistant":
                prompt_parts.append(f"<assistant>\n{msg.content}\n</assistant>")

        prompt_parts.append("<assistant>")
        return "\n\n".join(prompt_parts)

    def _phi_prompt(self, messages: List[ChatMessage]) -> str:
        """Phi ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        prompt_parts = []

        for msg in messages:
            if msg.role == "system":
                prompt_parts.append(f"System: {msg.content}")
            elif msg.role == "user":
                prompt_parts.append(f"User: {msg.content}")
            elif msg.role == "assistant":
                prompt_parts.append(f"Assistant: {msg.content}")

        return "\n".join(prompt_parts)

    def _tinyllama_prompt(self, messages: List[ChatMessage]) -> str:
        """TinyLlama ìµœì í™” í”„ë¡¬í”„íŠ¸"""
        if messages:
            last_msg = messages[-1]
            if last_msg.role == "user":
                return last_msg.content

        return self._llama_prompt(messages)

    def _llama_prompt(self, messages: List[ChatMessage]) -> str:
        """ê¸°ë³¸ Llama í”„ë¡¬í”„íŠ¸"""
        prompt_parts = []

        for msg in messages:
            if msg.role == "system":
                prompt_parts.append(f"System: {msg.content}")
            elif msg.role == "user":
                prompt_parts.append(f"User: {msg.content}")
            elif msg.role == "assistant":
                prompt_parts.append(f"Assistant: {msg.content}")

        return "\n\n".join(prompt_parts)

    def validate_config(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì¦"""
        return self._check_ollama_installed()

    def get_supported_features(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ëª©ë¡"""
        return [
            "chat",
            "streaming",
            "local_execution",
            "model_listing",
            "model_switching",
            "model_optimization",
            "server_auto_start",
            "cli_integration"
        ]

    def __del__(self):
        """ê°ì²´ ì†Œë©¸ ì‹œ ì„œë²„ ì •ë¦¬"""
        if hasattr(self, 'server_process') and self.server_process:
            self._stop_ollama_server()