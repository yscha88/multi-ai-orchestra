# ===== core/control/orchestrator_coordinator.py =====
"""
ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¡°ìœ¨ê¸° - Phase 1 ê¸°ë³¸ êµ¬í˜„
"""

import time
from typing import Dict, Any
from ports.control_ports import OrchestrationService
from ports.ai_ports import AIProvider, ModelSelector
from core.shared.models import (
    TaskAnalysis, OrchestratorResponse, SessionContext, OrchestratorType
)


class BasicOrchestratorCoordinator(OrchestrationService):
    """ê¸°ë³¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì„œë¹„ìŠ¤ (Phase 1)"""

    def __init__(self, ai_providers: Dict[str, AIProvider], model_selector: ModelSelector):
        self.ai_providers = ai_providers
        self.model_selector = model_selector
        self.processing_sessions = {}  # session_id -> processing_info

    def coordinate_processing(self, user_input: str, context: SessionContext,
                              task_analysis: TaskAnalysis) -> OrchestratorResponse:
        """ì²˜ë¦¬ ì¡°ìœ¨ (Phase 1: ê¸°ë³¸ êµ¬í˜„)"""
        start_time = time.time()

        # 1. ìµœì  í”„ë¡œë°”ì´ë” ì„ íƒ
        provider_name = self.select_optimal_provider(task_analysis)
        provider = self.ai_providers.get(provider_name)

        if not provider:
            raise RuntimeError(f"Provider '{provider_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # 2. ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self._start_monitoring(context.session_id, task_analysis, provider_name)

        try:
            # 3. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë©”ì‹œì§€ êµ¬ì„±
            messages = self._build_context_messages(user_input, context, task_analysis)

            # 4. AI í˜¸ì¶œ
            chat_response = provider.chat(messages)

            # 5. ì‘ë‹µ í›„ì²˜ë¦¬
            processed_response = self._post_process_response(
                chat_response, task_analysis, context
            )

            processing_time = time.time() - start_time

            # 6. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‘ë‹µ ìƒì„±
            orchestrator_response = OrchestratorResponse(
                content=processed_response,
                orchestrator_type=task_analysis.recommended_orchestrator,
                processing_time=processing_time,
                token_usage=chat_response.token_usage,
                task_analysis=task_analysis,
                used_providers=[provider_name],
                metadata={
                    "provider_info": provider.get_model_info().name,
                    "complexity": task_analysis.complexity.value,
                    "estimated_vs_actual_time": {
                        "estimated": task_analysis.estimated_time,
                        "actual": processing_time
                    }
                }
            )

            # 7. ëª¨ë‹ˆí„°ë§ ì™„ë£Œ
            self._complete_monitoring(context.session_id, True, orchestrator_response)

            return orchestrator_response

        except Exception as e:
            processing_time = time.time() - start_time
            self._complete_monitoring(context.session_id, False, None)
            raise RuntimeError(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def select_optimal_provider(self, task_analysis: TaskAnalysis) -> str:
        """ìµœì  í”„ë¡œë°”ì´ë” ì„ íƒ"""
        # Phase 1: ê°„ë‹¨í•œ ì„ íƒ ë¡œì§

        # ë³µì¡ë„ ê¸°ë°˜ ì„ íƒ
        if task_analysis.complexity == TaskComplexity.COMPLEX:
            # ë³µì¡í•œ ì‘ì—…ì€ ê³ ì„±ëŠ¥ ëª¨ë¸ (Claude)
            return "claude"

        # í•„ìš” ê¸°ëŠ¥ ê¸°ë°˜ ì„ íƒ
        if "code_generation" in task_analysis.required_capabilities:
            # ì½”ë“œ ìƒì„±ì€ CodeLlama ìš°ì„ 
            if "ollama" in self.ai_providers:
                ollama_provider = self.ai_providers["ollama"]
                if hasattr(ollama_provider, 'get_model_names'):
                    available_models = ollama_provider.get_model_names()
                    if any("codellama" in model.lower() for model in available_models):
                        return "ollama"

        # ê¸°ë³¸ê°’: ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ í”„ë¡œë°”ì´ë”
        for provider_name, provider in self.ai_providers.items():
            if provider.is_available():
                return provider_name

        raise RuntimeError("ì‚¬ìš© ê°€ëŠ¥í•œ AI Providerê°€ ì—†ìŠµë‹ˆë‹¤")

    def monitor_processing(self, session_id: str) -> Dict[str, Any]:
        """ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§"""
        if session_id not in self.processing_sessions:
            return {"status": "not_found"}

        session_info = self.processing_sessions[session_id]
        current_time = time.time()
        elapsed_time = current_time - session_info["start_time"]

        return {
            "status": session_info["status"],
            "elapsed_time": elapsed_time,
            "estimated_time": session_info["estimated_time"],
            "progress": min(elapsed_time / session_info["estimated_time"], 1.0),
            "provider": session_info["provider"],
            "complexity": session_info["complexity"]
        }

    # ===== í—¬í¼ ë©”ì„œë“œë“¤ =====

    def _build_context_messages(self, user_input: str, context: SessionContext,
                                task_analysis: TaskAnalysis) -> List[Any]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë©”ì‹œì§€ êµ¬ì„±"""
        from core.shared.models import ChatMessage

        messages = []

        # 1. ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° íƒ€ì…ì— ë”°ë¼)
        system_message = self._build_system_message(context, task_analysis)
        if system_message:
            messages.append(ChatMessage(role="system", content=system_message))

        # 2. ìµœê·¼ ëŒ€í™” ê¸°ë¡ (ì œí•œì ìœ¼ë¡œ)
        recent_turns = context.conversation_history[-3:]  # ìµœê·¼ 3í„´ë§Œ
        for turn in recent_turns:
            messages.append(ChatMessage(role="user", content=turn.user_message))
            messages.append(ChatMessage(role="assistant", content=turn.assistant_message))

        # 3. ê´€ë ¨ ê¸°ì–µ (ìˆë‹¤ë©´)
        if context.relevant_memories:
            memory_context = self._build_memory_context(context.relevant_memories)
            if memory_context:
                messages.append(ChatMessage(role="system", content=f"ê´€ë ¨ ê¸°ì–µ: {memory_context}"))

        # 4. í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
        messages.append(ChatMessage(role="user", content=user_input))

        return messages

    def _build_system_message(self, context: SessionContext, task_analysis: TaskAnalysis) -> str:
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµ¬ì„±"""
        user_profile = context.user_profile

        parts = [
            "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê°œì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
            f"ì‚¬ìš©ì ì •ë³´: {user_profile.name}, {user_profile.coding_style}, ì„ í˜¸ ì–¸ì–´: {', '.join(user_profile.preferred_languages)}"
        ]

        # ì‘ì—… ë³µì¡ë„ì— ë”°ë¥¸ ì§€ì¹¨
        if task_analysis.complexity == TaskComplexity.COMPLEX:
            parts.append("ë³µì¡í•œ ì‘ì—…ì´ë¯€ë¡œ ì²´ê³„ì ì´ê³  ë‹¨ê³„ë³„ë¡œ ì ‘ê·¼í•´ì£¼ì„¸ìš”.")
        elif task_analysis.complexity == TaskComplexity.MODERATE:
            parts.append("ì ë‹¹í•œ ë³µì¡ë„ì˜ ì‘ì—…ì´ë¯€ë¡œ ì‹¤ìš©ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•´ì£¼ì„¸ìš”.")
        else:
            parts.append("ê°„ë‹¨í•œ ì§ˆë¬¸ì´ë¯€ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.")

        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ìŠ¤íƒ€ì¼ ë°˜ì˜
        if user_profile.interaction_style == "brief":
            parts.append("ì‚¬ìš©ìëŠ” ê°„ê²°í•œ ë‹µë³€ì„ ì„ í˜¸í•©ë‹ˆë‹¤.")
        elif user_profile.interaction_style == "detailed":
            parts.append("ì‚¬ìš©ìëŠ” ìƒì„¸í•œ ì„¤ëª…ì„ ì„ í˜¸í•©ë‹ˆë‹¤.")

        return " ".join(parts)

    def _build_memory_context(self, memories: List[Any]) -> str:
        """ê¸°ì–µ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not memories:
            return ""

        memory_summaries = []
        for memory in memories[:2]:  # ìµœëŒ€ 2ê°œë§Œ
            summary = memory.content[:100] + "..." if len(memory.content) > 100 else memory.content
            memory_summaries.append(summary)

        return " | ".join(memory_summaries)

    def _post_process_response(self, chat_response: Any, task_analysis: TaskAnalysis,
                               context: SessionContext) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        response_content = chat_response.content

        # Phase 1: ê¸°ë³¸ì ì¸ í›„ì²˜ë¦¬ë§Œ

        # 1. ì‚¬ìš©ì ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ì¡°ì •
        if context.user_profile.interaction_style == "brief":
            # ë„ˆë¬´ ê¸´ ì‘ë‹µì€ ìš”ì•½ ì œì•ˆ
            if len(response_content) > 500:
                response_content += "\n\nğŸ’¡ ë” ê°„ë‹¨í•œ ì„¤ëª…ì´ í•„ìš”í•˜ì‹œë©´ 'ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜'ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”."

        # 2. ë³µì¡í•œ ì‘ì—…ì˜ ê²½ìš° ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
        if task_analysis.complexity == TaskComplexity.COMPLEX:
            response_content += "\n\nğŸ”„ ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ë¶€ë¶„ì´ë‚˜ êµ¬ì²´ì ìœ¼ë¡œ ì§„í–‰í•˜ê³  ì‹¶ì€ ë‹¨ê³„ê°€ ìˆìœ¼ë©´ ë§ì”€í•´ì£¼ì„¸ìš”."

        return response_content

    def _start_monitoring(self, session_id: str, task_analysis: TaskAnalysis, provider_name: str):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.processing_sessions[session_id] = {
            "status": "processing",
            "start_time": time.time(),
            "estimated_time": task_analysis.estimated_time,
            "provider": provider_name,
            "complexity": task_analysis.complexity.value
        }

    def _complete_monitoring(self, session_id: str, success: bool, response: Any):
        """ëª¨ë‹ˆí„°ë§ ì™„ë£Œ"""
        if session_id in self.processing_sessions:
            self.processing_sessions[session_id]["status"] = "completed" if success else "failed"
            self.processing_sessions[session_id]["end_time"] = time.time()

            # ì¼ì • ì‹œê°„ í›„ ì •ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
            # TODO: ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—… êµ¬í˜„